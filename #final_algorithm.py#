from collections import defaultdict
from IPython.display import display
from matplotlib import dates

from model.Base import Base
from model.User import User
from model.Device import Device
from model.HttpReq import HttpReq
from model.DnsReq import DnsReq
from model.user_devices import user_devices;

from sqlalchemy import create_engine, text, func
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import NullPool

from blacklist import create_blacklist_dict
from blacklist import is_in_blacklist
from inter_event_time_by_url_analysis import filter_spike
from Traces import Trace

import datautils
import datetime
import matplotlib.pyplot as plt
import seaborn as sns

DB='postgresql+psycopg2:///ucnstudy'

engine = create_engine(DB, echo=False, poolclass=NullPool)
Base.metadata.bind = engine
Session = sessionmaker(bind=engine)

ses = Session()
users = ses.query(User)

def final_algorithm_filtered_traces(f_blacklist, f_seconds, f_spikes):
"""
Filters out background activity and create file with remaining traces.
Parameters:
- f_blacklist (bool): indicates if packets will be filtered by blacklist.
- f_seconds (bool): indicates if packets will be filtered by interval smaller than a second.
- f_spikes (bool): indicates if packets will be filtered by periodic intervals.
"""
    blacklist = create_blacklist_dict()
    filtered_traces_user_dict = defaultdict(list)

    file_type = get_file_type(f_blacklist, f_seconds, f_spikes)

    #sliding windows for creating packets intervals.
    bucket_list = [1, 5, 10, 30, 60]

    traces_file_1 = open('traces_bucket_1_%s'%(file_type), 'w')
    traces_file_5 = open('traces_bucket_5_%s'%(file_type), 'w')
    traces_file_10 = open('traces_bucket_10_%s'%(file_type), 'w')
    traces_file_30 = open('traces_bucket_30_%s'%(file_type), 'w')
    traces_file_60 = open('traces_bucket_60_%s'%(file_type), 'w')

    block_length = 60*5

    for user in users:
        devids = []
        for d in user.devices:
            devids.append(str(d.id))

        devs = {}
        for d in user.devices:
            devs[d.id] = d.platform

        for elem_id in devids:
            sql_userid = """SELECT login FROM devices WHERE id =:d_id"""
            user_id = ses.execute(text(sql_userid).bindparams(d_id = elem_id)).fetchone()
            idt = user_id[0]

            #gets only users that have ground truth data.
            if idt != 'bowen.laptop' and idt != 'bridgeman.laptop2' and idt != 'bridgeman.stuartlaptop' and idt != 'chrismaley.loungepc' and idt != 'chrismaley.mainpc' and idt != 'clifford.mainlaptop' and idt != 'gluch.laptop' and idt != 'kemianny.mainlaptop' and idt != 'neenagupta.workpc':
                continue

            print idt

            http_traces_list, dns_traces_list = get_test_data(elem_id)
            
            filtered_http_traces = filter_traces(block_length, http_traces_list, blacklist, f_blacklist, f_seconds, f_spikes)
            filtered_dns_traces = filter_traces(block_length, dns_traces_list, blacklist, f_blacklist, f_seconds, f_spikes)

            for key, timsts in filtered_http_traces.iteritems():
                for timst in timsts:
                    filtered_traces_user_dict[idt].append(timst)

            for key, timsts in filtered_dns_traces.iteritems():
                for timst in timsts:
                    filtered_traces_user_dict[idt].append(timst)

            #plot_traces(filtered_http_traces, filtered_dns_traces, idt)
            #plot_traces(filtered_traces_user_dict[idt], idt)

            for bucket in bucket_list:
                traces_bucket = []
                traces_bucket = get_interval_list_predefined_gap(sorted(filtered_traces_user_dict[idt]), bucket)
                if bucket == 1:
                    traces_file_1.write('\n' + idt)
                elif bucket == 5:
                    traces_file_5.write('\n' + idt)
                elif bucket == 10:
                    traces_file_10.write('\n' + idt)
                elif bucket == 30:
                    traces_file_30.write('\n' + idt)
                elif bucket == 60:
                    traces_file_60.write('\n' + idt)

                print len(traces_bucket)
                for timst in traces_bucket:
                    if bucket == 1:
                        traces_file_1.write('\n' +str(timst))
                    elif bucket == 5:
                        traces_file_5.write('\n' +str(timst))
                    elif bucket == 10:
                        traces_file_10.write('\n' +str(timst))
                    elif bucket == 30:
                        traces_file_30.write('\n' +str(timst))
                    elif bucket == 60:
                        traces_file_60.write('\n' +str(timst))

    traces_file_1.close()
    traces_file_5.close()
    traces_file_10.close()
    traces_file_30.close()
    traces_file_60.close()
    
    return filtered_traces_user_dict


def get_file_type(f_blacklist, f_seconds, f_spikes):
"""
Defines the name for the type of filtering that will be done.
- f_blacklist (bool): indicates if packets will be filtered by blacklist.
- f_seconds (bool): indicates if packets will be filtered by interval smaller than a second.
- f_spikes (bool): indicates if packets will be filtered by periodic intervals.
Returns:
- string containing filtering type.
"""
    if f_blacklist and f_seconds and f_spikes:
        return 'filtered'

    elif f_blacklist and not f_seconds and not f_spikes:
        return 'blist_filtered'

    elif not f_blacklist and f_seconds and f_spikes:
        return 'interval_filtered'

    elif not f_blacklist and not f_seconds and not f_spikes:
        return 'not_filtered'


def get_interval_list_predefined_gap(traces_list, gap_interval):
"""
Creates extra packets, based on the length of the sliding window.
Parameters:
- traces_list(list of timestamps): contains times of packets that were requested.
- gap_interval(int): sliding window length.
Returns:
- interval_list(list of timestamps): list containing previous and new timestamps. 
"""
    intv = 0
    interval_list = []
    pre_traces = []

    for timst in traces_list:
        timst = timst.replace(microsecond=0)
        pre_traces.append(timst)

    for i in range(0, len(pre_traces)-1):
        iat = (pre_traces[i+1]-pre_traces[i]).total_seconds()
        if iat <= gap_interval:
            current_trace = pre_traces[i]
            while current_trace < pre_traces[i+1]:
                interval_list.append(current_trace)
                current_trace = current_trace + datetime.timedelta(0,1)
        else:
            interval_list.append(pre_traces[i])

        if i == len(pre_traces)-2:
            interval_list.append(pre_traces[i+1])

    return interval_list


def get_test_data(device_id):
"""
Gets dns and http requests packets from the database.
Parameters:
- device_id(int): unique device identifier.
Returns:
- http_traces_list (list of Traces): contains http request packets for a device.
- dns_traces_list (list of Traces): contains dns request packets for a device.
"""

    sql_http = """SELECT req_url_host, ts, lag(ts) OVER (ORDER BY ts) FROM httpreqs2 \
        WHERE devid =:d_id AND matches_urlblacklist = 'f' and source = 'hostview'"""

    sql_dns = """SELECT query, ts, lag(ts) OVER (ORDER BY ts) FROM dnsreqs \
        WHERE devid =:d_id AND matches_blacklist = 'f'"""

    http_traces_list = []
    for row in ses.execute(text(sql_http).bindparams(d_id = device_id)):
        elem = Trace(row[0], row[1])
        http_traces_list.append(elem)

    dns_traces_list = []
    for row in ses.execute(text(sql_dns).bindparams(d_id = device_id)):
        elem = Trace(row[0], row[1])
        dns_traces_list.append(elem)

    return http_traces_list, dns_traces_list


def filter_traces(block_length, traces_list, blacklist, filter_blist, filter_iat, filter_spike):
"""
Filter out packets from a given list of packets
Parameters:
- blocklength(int): length of the interval whose packets will be filtered.
- traces_list(list of Traces): list containing object with timestamp and url.
- blacklist(dictionary of strings): contains list of blacklists to be filtered.
- filter_blist (bool): indicates if packets will be filtered by blacklist.
- filter_iat (bool): indicates if packets will be filtered by interval smaller than a second.
- filter_spike (bool): indicates if packets will be filtered by periodic intervals.
Returns:
- filtered_url_traces(dictionary of url): contains filtered timestamps per url.
"""
    #print datetime.timedelta(0,block_length)
    i = 0
    filtered_url_traces = defaultdict(list)
    while i < len(traces_list):
        block_url_dict = defaultdict(list)
        filtered_block_url_dict = defaultdict(list)
        elem = traces_list[i]
        beg_block =elem.timst
        prev_pos = i
        #creates blocks of url filtered by blacklist
        while elem.timst <= beg_block + datetime.timedelta(0,block_length):
            if i >= len(traces_list):
                break
            if filter_blist: 
                if elem.url_domain and not (is_in_blacklist(elem.url_domain, blacklist)):
                    block_url_dict[elem.url_domain].append(elem)
            else:
                block_url_dict[elem.url_domain].append(elem)
            elem = traces_list[i]
            i +=1

        #filters by iat < 1
        for key, values in block_url_dict.iteritems():
            if filter_iat:
                for j in range(0, len(values)-1):
                    current_trace = values[j]
                    next_trace = values[j+1]
                    iat = (next_trace.timst - current_trace.timst).total_seconds()
                    if filter_iat:
                        if iat > 1:
                            filtered_block_url_dict[key].append(current_trace.timst)
                            if j+1 == len(values) - 1:
                                filtered_block_url_dict[key].append(next_trace.timst)
            else:
                for elem in values:
                    filtered_block_url_dict[key].append(elem.timst)

            #filters by spike
            if filter_spike and filtered_block_url_dict[key] and len(filtered_block_url_dict[key]) > 1:
                filtered_block_url_dict[key], deleted_url = filter_spikes(filtered_block_url_dict[key], key, [])

            for elem in filtered_block_url_dict[key]:
                filtered_url_traces[key].append(elem)

        #only one element
        if prev_pos == i:
            i += 1

    return filtered_url_traces


def get_daily_traces(traces, bucket_beg):
    daily_list = []
    for elem in traces:
        if elem.date() == bucket_beg.date():
            daily_list.append(elem)

    return daily_list


def plot_traces(traces_dict, user_id):
    x = []
    y = []
    sns.set(style='whitegrid')
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))

    for timst in traces_dict:
        x.append(timst.hour+timst.minute/60.0+timst.second/3600.0)
        y.append(timst.date())

    """for key, timsts in http_traces_dict.iteritems():
        for timst in timsts:
            x.append(timst.hour+timst.minute/60.0+timst.second/3600.0)
            y.append(timst.date())

    for key, timsts in dns_traces_dict.iteritems():
        for timst in timsts:
            x.append(timst.hour+timst.minute/60.0+timst.second/3600.0)
            y.append(timst.date())"""

    y_label = list(set(y))
    hfmt = dates.DateFormatter('%m-%d')
    ax.yaxis.set_major_formatter(hfmt)

    ax.plot(x,y, '.g')

    ax.set_title('Device usage [user=%s]'%(user_id), fontsize = 30)#, device=%s]'%(username, platform))
    ax.set_ylabel('Date')
    ax.set_yticks(y_label)

    ax.set_xlabel('Device Activity')
    ax.set_xlim(0,24)

    plt.tight_layout()
    #fig.savefig('figs_device_constant_usage_filtered/%s.png' % (user_id))
    #plt.close(fig)

    plt.show(fig)


if __name__ == '__main__':
    final_algorithm_filtered_traces(True, True, True)
